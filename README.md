# ⚡ SkillProof

> AI-powered developer evaluation platform. Submit a GitHub project, get a multi-dimensional score, and generate copy-paste resume bullets — all in under 30 seconds.

![Tech Stack](https://img.shields.io/badge/stack-React%20%7C%20Node.js%20%7C%20MongoDB%20%7C%20Groq%20LLM-6C63FF?style=flat-square)
![Phase](https://img.shields.io/badge/phase-2%20complete-success?style=flat-square)

---

## What It Does

- **AI Evaluation** — 5-dimension scoring (Architecture, Code Quality, Scalability, Innovation, Real-World Impact) via Groq LLM
- **Company-Fit Scoring** — Google / Startup / Enterprise fit percentages per project
- **Resume Bullets** — Copy-paste ready resume lines generated by AI, specific to each project
- **Leaderboard** — Public ranking of top developers by score
- **Project Comparison** — Side-by-side scoring breakdown
- **Public Developer Profiles** — Shareable profile pages at `/u/:slug`
- **Score History** — Visual trend chart across all your evaluated projects

---

## Tech Stack

| Layer | Technology |
|---|---|
| Frontend | React 18, Vite, Recharts |
| Backend | Node.js, Express 5 |
| Database | MongoDB + Mongoose |
| AI | Groq SDK (`llama-3.3-70b-versatile`) |
| Auth | JWT (jsonwebtoken + bcryptjs) |
| Logging | Winston |
| Containerization | Docker + Docker Compose |

---

## Local Setup

### Prerequisites
- Node.js 20+
- MongoDB (local or Atlas)
- Groq API key ([free at console.groq.com](https://console.groq.com))

### 1. Clone & install

```bash
git clone https://github.com/your-username/skillproof.git
cd skillproof

# Backend
cd backend && npm install

# Frontend
cd ../frontend && npm install
```

### 2. Configure environment

```bash
cp backend/.env.example backend/.env
# Fill in MONGO_URI, JWT_SECRET, GROQ_API_KEY
```

### 3. Start

```bash
# Terminal 1 — backend (port 5001)
cd backend && npm run dev

# Terminal 2 — frontend (port 5173)
cd frontend && npm run dev
```

### 4. Docker (full stack)

```bash
docker-compose up --build
# Backend: http://localhost:5001
# MongoDB: mongodb://localhost:27017
```

---

## API Endpoints

| Method | Route | Auth | Description |
|---|---|---|---|
| GET | `/health` | No | Health check + DB status |
| POST | `/api/auth/register` | No | Create account |
| POST | `/api/auth/login` | No | Login, returns JWT |
| POST | `/api/projects` | JWT | Submit + evaluate project |
| GET | `/api/projects` | JWT | List user's projects (paginated) |
| GET | `/api/projects/:id` | JWT | Project detail + evaluation |
| GET | `/api/leaderboard` | No | Top developers (paginated) |
| GET | `/api/profile/:slug` | No | Public developer profile |

---

## Scoring Model

Final score is **server-computed** (not AI-determined) using a deterministic weighted formula:

```
finalScore = (
  architectureScore    × 0.25 +
  codeQualityScore     × 0.25 +
  scalabilityScore     × 0.20 +
  innovationScore      × 0.15 +
  realWorldImpactScore × 0.15
) × 10

Result: 0–100 integer
```

This is versioned (`SCORE_VERSION: v1.1`) and fully auditable.

---

## Architecture

See [`docs/architecture.md`](docs/architecture.md) for:
- Full system diagram
- Request lifecycle walkthrough
- AI evaluation flow + prompt versioning
- Scale-to-1M strategy
- Caching + rate limiting design
- Failure recovery model

---

## Deployment

### Backend → Render / Railway

```
Build: npm install
Start: node server.js

Env vars: PORT, NODE_ENV, MONGO_URI, JWT_SECRET, GROQ_API_KEY, CORS_ORIGIN
```

### Frontend → Vercel

```
Build: npm run build
Output: dist

Env: VITE_API_URL=https://your-backend.onrender.com/api
```

Update `frontend/src/api/axios.js` to use `import.meta.env.VITE_API_URL` as the baseURL for production.

---

## Resume Bullets

```
• Built an AI-powered developer evaluation platform using Node.js and Groq LLM,
  returning multi-dimensional project scores and copy-paste resume bullets in <30s

• Designed a deterministic scoring engine decoupled from the AI layer,
  enabling reproducible 0–100 scores across model upgrades

• Implemented leaderboard with MongoDB aggregation pipeline — per-user best score,
  average score, compound indexes, and paginated responses

• Integrated full AI metadata tracking per evaluation: model version, prompt version,
  token usage, confidence score — supporting cost attribution and auditability

• Architected backend with layered separation (routes → controllers → services → utils),
  centralized error handling, graceful shutdown, and Winston structured logging
```

---

## Interview Talking Points

**"Walk me through how the AI scoring works."**
> The user submits a project. We call Groq's LLM with a structured system prompt (versioned as `PROMPT_VERSION`). The AI returns raw sub-scores for 5 dimensions. Our server then applies a deterministic weighted formula to compute the final 0–100 score. The AI never directly determines the final score — this makes results reproducible and auditable.

**"How would you scale this to 1M users?"**
> Move AI evaluation to an async queue (BullMQ + Redis). Shard MongoDB by `userId`. Cache the leaderboard and public profiles in Redis with short TTLs. Scale Express horizontally behind a load balancer — it's stateless (JWT auth, no sessions). See `docs/architecture.md` for the full breakdown.

**"How do you handle AI failures?"**
> Every AI call has a 30-second `AbortController` timeout. If the call fails or times out, we return a deterministic fallback (all zeros, `fallback: true` flag on the evaluation). The project is saved with `status: "failed"` and the user can resubmit. No data is lost.
