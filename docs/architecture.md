# SkillProof â€” System Architecture

## Overview

SkillProof is an AI-powered developer evaluation platform. Developers submit GitHub projects, receive multi-dimensional AI scoring, and can share their results publicly. Built with React + Vite (frontend), Node.js + Express (backend), MongoDB (data), and Groq LLM (AI).

---

## Architecture Diagram

```mermaid
graph TD
    U[ðŸ‘¤ User Browser] -->|HTTPS| FE[React + Vite Frontend\nVercel]
    FE -->|REST API| BE[Node.js + Express Backend\nRender / Railway]
    BE -->|Mongoose ODM| DB[(MongoDB Atlas)]
    BE -->|Groq SDK| AI[Groq LLM API\nllama-3.3-70b]
    FE -->|/u/:slug| PP[Public Profile\nno auth required]
    FE -->|/leaderboard| LB[Leaderboard\nno auth required]
```

---

## Request Lifecycle â€” Project Evaluation

```
1. POST /api/projects (authenticated)
   â”‚
   â”œâ”€â”€ 1. Auth middleware verifies JWT, attaches user to req
   â”œâ”€â”€ 2. express-validator validates title, githubUrl, description
   â”œâ”€â”€ 3. Project saved to DB with status = "processing"
   â”œâ”€â”€ 4. AI evaluation called (Groq SDK, 30s timeout)
   â”‚       â””â”€â”€ Returns 5 dimension scores + company-fit + resume bullets
   â”œâ”€â”€ 5. calculateFinalScore() â€” deterministic weighted formula (server-owned)
   â”œâ”€â”€ 6. Project updated with scores, AI metadata, status = "evaluated"
   â”œâ”€â”€ 7. User.aiTokensUsed incremented with token count
   â””â”€â”€ 8. 201 response with full project object
```

**Why is the final score server-computed?**
- Reproducibility â€” changing AI model doesn't change the scoring formula
- Security â€” clients cannot manipulate scores
- Auditability â€” formula is versioned (`SCORE_VERSION`) and documented

---

## AI Evaluation Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ System Prompt (PROMPT_VERSION: v2.1)                â”‚
â”‚ - Evaluation rubric (1â€“10 per dimension)            â”‚
â”‚ - Company-fit definitions (google/startup/mnc)      â”‚
â”‚ - Format: strict JSON schema                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ User Prompt                                         â”‚
â”‚ - Project title, description, GitHub URL, techStack â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Groq (llama-3.3-70b-versatile)                     â”‚
â”‚ response_format: json_object                        â”‚
â”‚ temperature: 0.2 (low for consistency)              â”‚
â”‚ timeout: 30s with AbortController                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
  Fallback if AI fails: zeros + fallback:true flag
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ scoreCalculator.js â€” deterministic formula          â”‚
â”‚ finalScore = weighted(arch*0.25 + scale*0.20 +      â”‚
â”‚   quality*0.25 + innov*0.15 + impact*0.15) * 10     â”‚
â”‚ Result: 0â€“100 integer                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Stored with every evaluation:**
| Field | Purpose |
|---|---|
| `aiModelVersion` | Reproducibility â€” which model scored this |
| `promptVersion` | Reproducibility â€” which prompt was used |
| `tokenUsage` | Cost tracking per evaluation |
| `confidenceScore` | AI self-reported confidence (0â€“100) |
| `evaluatedAt` | Timestamp for debugging |
| `fallback` | True if AI failed and zeros were returned |

---

## Database Schema â€” Key Collections

### Users
```
_id, name, email, password (bcrypt), role
publicProfileSlug, publicProfileEnabled, bio, avatarUrl
aiTokensUsed, aiCostUsd
achievements[]
```
**Indexes:** `{ email: 1 }`, `{ publicProfileSlug: 1 }`

### Projects
```
_id, user (ref), title, githubUrl, description, techStack[]
status: pending | processing | evaluated | failed
finalScore (0â€“100), evaluation { ...scores, companyFit, resumeBullets, metadata }
isPublic, timestamps
```
**Indexes:**
- `{ user: 1, createdAt: -1 }` â€” user's project list
- `{ finalScore: -1 }` â€” leaderboard sort
- `{ status: 1, finalScore: -1 }` â€” leaderboard filter
- `{ createdAt: -1 }` â€” global feed

---

## Scaling Strategy â€” 1M Users

### Current State (0â€“10K users)
- Single Node.js process on Render/Railway
- MongoDB Atlas M0 free tier â†’ M10 as needed
- Groq API for AI (serverless, no infra)

### 10K â†’ 100K Users
```
Load Balancer (Render/Railway auto-scale)
    â”œâ”€â”€ Node instance 1
    â”œâ”€â”€ Node instance 2
    â””â”€â”€ Node instance N (stateless â€” JWT, no sessions)

MongoDB Atlas M30 with replica set
Redis (Upstash) for:
    â”œâ”€â”€ Leaderboard cache (5-min TTL)
    â”œâ”€â”€ Public profile cache (1-min TTL)
    â””â”€â”€ Rate limit counters
```

### 100K â†’ 1M Users
```
API Gateway (AWS API GW or Cloudflare)
    â”œâ”€â”€ Auth Service (lightweight â€” JWT verify)
    â”œâ”€â”€ Project Service (CRUD + metadata)
    â”œâ”€â”€ AI Evaluation Worker (async queue)
    â”‚       â””â”€â”€ BullMQ + Redis for job queue
    â”‚           â””â”€â”€ Worker pulls jobs, calls Groq, updates DB
    â””â”€â”€ Profile/Leaderboard Service (read-heavy, cached)

MongoDB Atlas M50+ with sharding on userId
CDN (Cloudflare) for static assets + public profiles
```

---

## Caching Strategy

| Resource | TTL | Strategy |
|---|---|---|
| Leaderboard | 5 min | Server-side Redis, invalidate on new evaluation |
| Public profile | 1 min | Redis key by slug |
| Static assets | 1 year | Vercel CDN (content-hashed filenames) |
| AI responses | Not cached | Non-deterministic, project-specific |

---

## Rate Limiting

| Route | Window | Max Requests | Rationale |
|---|---|---|---|
| `/api/auth/*` | 15 min | 20 | Brute-force protection |
| `/api/projects` | 60 min | 10 | AI calls cost money |
| All other routes | 15 min | 200 | General abuse prevention |

Production: move to Redis-backed rate limiter (e.g., `rate-limit-redis`) for consistency across multiple instances.

---

## AI Cost Optimization

1. **Model selection**: `llama-3.3-70b-versatile` via Groq is free/cheap vs GPT-4
2. **Temperature 0.2**: Low temperature = shorter, more deterministic outputs = fewer tokens
3. **JSON mode**: `response_format: json_object` prevents long prose responses
4. **Token tracking**: Every evaluation records `tokenUsage` on the project document
5. **User-level tracking**: `user.aiTokensUsed` for cost attribution
6. **Timeout**: 30s AbortController prevents runaway requests
7. **Future**: Cache evaluations for same GitHub URL + description hash

---

## Failure Recovery

| Failure | Behavior |
|---|---|
| Groq API down | Returns fallback evaluation (zeros, `fallback:true`), project saved as `failed` |
| Groq timeout (30s) | Same as above, AbortController fires |
| MongoDB down | Express 503 via errorHandler |
| Invalid AI JSON | `JSON.parse` throws â†’ caught â†’ fallback returned |
| Server crash | Graceful SIGTERM handler closes DB connection cleanly |

---

## Security Model

- **JWT auth** â€” `HS256`, 7-day expiry, stored in localStorage (upgrade to httpOnly cookie for production)
- **Helmet.js** â€” sets 11 security headers
- **CORS** â€” explicit allowlist from `CORS_ORIGIN` env var
- **Input validation** â€” express-validator on all write routes
- **Password hashing** â€” bcryptjs, salt rounds 10
- **Ownership checks** â€” every project read/write verifies `project.user === req.user._id`
- **Rate limiting** â€” per-route limits on auth and eval endpoints

---

## Deployment

### Backend (Render / Railway)
```bash
# Build command
npm install

# Start command
node server.js

# Environment variables (set in dashboard)
PORT, NODE_ENV, MONGO_URI, JWT_SECRET, GROQ_API_KEY, CORS_ORIGIN
```

### Frontend (Vercel)
```bash
# Build command
npm run build

# Output directory
dist

# Environment variable
VITE_API_URL=https://your-backend.onrender.com/api
```

> Also update `frontend/src/api/axios.js` baseURL to use `import.meta.env.VITE_API_URL` in production.

